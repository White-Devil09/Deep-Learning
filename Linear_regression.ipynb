{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Linear Regression in PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs ( temp, rainfall, humidity) and targets (yelid of apples and oranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73,67,43],\n",
    "                   [91,88,64],\n",
    "                   [87,134,58],\n",
    "                   [102,43,37],\n",
    "                   [69,96,70]], dtype=np.float32)\n",
    "\n",
    "target = np.array([[56,70],\n",
    "                   [81,101],\n",
    "                   [119,133],\n",
    "                   [22,37],\n",
    "                   [103,119]],dtype=np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting inputs and targets into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "target = torch.from_numpy(target)\n",
    "\n",
    "print(inputs)\n",
    "print(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8347, -0.8753, -0.2540],\n",
      "        [ 0.3690, -0.6541,  1.3285]], requires_grad=True)\n",
      "tensor([-1.9482,  0.0925], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2,3,requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lambda x : x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-10.5870,  40.3316],\n",
      "        [-19.2795,  61.1365],\n",
      "        [-61.3583,  21.6007],\n",
      "        [ 36.1504,  58.7603],\n",
      "        [-46.1691,  55.7565]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# generating predictions\n",
    "\n",
    "predicts = model(inputs)\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# comparing with targets\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(t1,t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff)/diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8882.2939, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = mse(predicts,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ -7705.3579, -10233.1416,  -5932.0371],\n",
       "         [ -3525.8726,  -5111.7920,  -2782.0159]]),\n",
       " tensor([-96.4487, -44.4829]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad,b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.9117, -0.7730, -0.1947],\n",
       "         [ 0.4043, -0.6030,  1.3563]], requires_grad=True),\n",
       " tensor([-1.9473,  0.0929], requires_grad=True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * lr\n",
    "    b -= b.grad * lr\n",
    "\n",
    "w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6644.0376, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now lets verify loss is lowered or not\n",
    "predicts = model(inputs)\n",
    "loss = mse(predicts,target)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you done with computation of gradient and again you want to do gradient calculation set grads to zero or else it will add new grads to previous ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([0., 0.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "w.grad,b.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model using gradient descent\n",
    "\n",
    "As seen above, we reduce the loss and improve our model using the gradient descent optimization algorithm. Thus, we can train the model using the following steps:\n",
    "\n",
    "1. Generate predictions\n",
    "2. Calculate the loss\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "5. Reset the gradients to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  4.4458,  47.5271],\n",
      "        [  0.5350,  70.6243],\n",
      "        [-37.5007,  33.1320],\n",
      "        [ 50.6059,  65.5845],\n",
      "        [-26.8752,  65.0446]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predicts = model(inputs)\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6644.0376, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "loss = mse(predicts,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients w.r.t weights and biases\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting weights by subtracting a small quantity proportional to the gradient\n",
    "lr = 1e-5\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * lr\n",
    "    b -= b.grad * lr\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With new weights and biases lets see how loss is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5127.5303, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "predicts = model(inputs)\n",
    "loss = mse(predicts, target)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model for multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "while loss > 0.6:\n",
    "    predicts = model(inputs)\n",
    "    loss = mse(predicts, target)\n",
    "    loss.backward()\n",
    "    lr = 1e-5\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * lr\n",
    "        b -= b.grad * lr\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5997, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Lets verify loss is lowered or not\n",
    "predicts = model(inputs)\n",
    "loss = mse(predicts, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 56.8589,  70.2928],\n",
       "         [ 82.2386, 100.9149],\n",
       "         [119.1072, 132.4281],\n",
       "         [ 21.2238,  36.8656],\n",
       "         [101.5075, 119.6675]], grad_fn=<AddBackward0>),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.],\n",
       "         [ 22.,  37.],\n",
       "         [103., 119.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts,target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using built-in PyTorch library for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "target = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "target = torch.from_numpy(target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TensorDataset(inputs,target)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TensirDataset` allow us to access a small section of the training data using the training data using the array indexing notation (`[0:3]` in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a batch loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 69.,  96.,  70.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 68.,  96.,  71.],\n",
      "        [ 73.,  66.,  44.],\n",
      "        [ 87., 135.,  57.]])\n",
      "tensor([[103., 119.],\n",
      "        [ 81., 101.],\n",
      "        [104., 118.],\n",
      "        [ 57.,  69.],\n",
      "        [118., 134.]])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "instead of initializing weights and biases manually, we can define the model using the `nn.Linear` class from PyTorch which does it automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1958, -0.0320,  0.2164],\n",
      "        [ 0.0178, -0.1479,  0.1627]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.5366,  0.0494], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Defining model\n",
    "\n",
    "model = nn.Linear(3,2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.1958, -0.0320,  0.2164],\n",
       "         [ 0.0178, -0.1479,  0.1627]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.5366,  0.0494], requires_grad=True)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters \n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -7.6661,  -1.5720],\n",
      "        [ -7.3170,  -0.9433],\n",
      "        [ -9.3051,  -8.7958],\n",
      "        [-13.8742,   1.5179],\n",
      "        [ -1.9673,  -1.5417],\n",
      "        [ -7.8299,  -1.4063],\n",
      "        [ -7.0686,  -0.6327],\n",
      "        [ -9.2845,  -8.6154],\n",
      "        [-13.7104,   1.3522],\n",
      "        [ -1.5551,  -1.3968],\n",
      "        [ -7.4177,  -1.2614],\n",
      "        [ -7.4808,  -0.7776],\n",
      "        [ -9.5536,  -9.1064],\n",
      "        [-14.2864,   1.3730],\n",
      "        [ -1.8035,  -1.7074]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using model\n",
    "\n",
    "predicts = model(inputs)\n",
    "print(predicts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of defining loss function manually we can use built-in loss function `mse_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9177.6045, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(model(inputs),target)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "instead of manually manipulating the model's weights and biases using gradients, we can use the optimizer `optim.SGD`. SGD is short for 'stochastic gradient descent'. The term stochastic indicates that samples are selected in random batches instead of as a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(),lr=1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: `model.parameters()` is passses as an argument to `optim.SGD` so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate that controls the amount by which the parameters are modified"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model \n",
    "\n",
    "We are now ready to train the model. We'll follow the same process to implement gradient descent:\n",
    "\n",
    "1. Generate predictions\n",
    "2. Calculate the loss\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "5. Reset the gradients to zero\n",
    "\n",
    "The only change is that we'll work on batch of data instead of processing the entire training data in every iteration. Let's define a utility function `fit` that trains the model for a given number of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to train modle\n",
    "def fit(num_epoch,model, loss_fn, opt, train_dl):\n",
    "\n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        # Train with batch of data\n",
    "        for xb,yb in train_dl:\n",
    "\n",
    "            # 1. Generate predictions\n",
    "            predicts = model(xb)\n",
    "\n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(predicts,yb)\n",
    "\n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "\n",
    "            # 5. Reset gradients to zero\n",
    "            opt.zero_grad()\n",
    "\n",
    "        # Printing the process\n",
    "        if((epoch+1)%10 == 0):\n",
    "            print(f'Epoch {epoch+1}/{num_epoch} --------- loss : {loss.item()}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note above:\n",
    "\n",
    "1. We use the data loader defined earlier to get batches of data for every iteration\n",
    "2. Instead of updating parameters (weights and biases) manually, we use `opt.step` to perform the update and `opt.zero_grad` to reset the gradients to zero.\n",
    "3. We've also addad a log statement that prints the loss from the last batch of data for every $10^{th}$ epoch to track the training progress. `loss.item()` returns the actual value stored in the loss tensor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 --------- loss : 543.476318359375\n",
      "Epoch 20/200 --------- loss : 124.69002532958984\n",
      "Epoch 30/200 --------- loss : 96.85050201416016\n",
      "Epoch 40/200 --------- loss : 61.642913818359375\n",
      "Epoch 50/200 --------- loss : 63.903038024902344\n",
      "Epoch 60/200 --------- loss : 61.39103317260742\n",
      "Epoch 70/200 --------- loss : 46.685543060302734\n",
      "Epoch 80/200 --------- loss : 27.779117584228516\n",
      "Epoch 90/200 --------- loss : 34.26868438720703\n",
      "Epoch 100/200 --------- loss : 21.014400482177734\n",
      "Epoch 110/200 --------- loss : 5.479875087738037\n",
      "Epoch 120/200 --------- loss : 13.077142715454102\n",
      "Epoch 130/200 --------- loss : 11.66965103149414\n",
      "Epoch 140/200 --------- loss : 8.120980262756348\n",
      "Epoch 150/200 --------- loss : 8.627596855163574\n",
      "Epoch 160/200 --------- loss : 4.261711120605469\n",
      "Epoch 170/200 --------- loss : 5.70742654800415\n",
      "Epoch 180/200 --------- loss : 1.9382864236831665\n",
      "Epoch 190/200 --------- loss : 3.3390235900878906\n",
      "Epoch 200/200 --------- loss : 4.266870498657227\n"
     ]
    }
   ],
   "source": [
    "fit(200,model,loss_fn,opt,train_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate predictions using out model and verify that they're close to our targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.0725,  70.6833],\n",
       "        [ 81.5809,  99.1757],\n",
       "        [119.1362, 135.2701],\n",
       "        [ 21.9023,  39.3514],\n",
       "        [100.3285, 115.2197],\n",
       "        [ 55.8385,  69.5995],\n",
       "        [ 81.3343,  99.0166],\n",
       "        [119.3856, 135.7386],\n",
       "        [ 23.1363,  40.4352],\n",
       "        [101.3158, 116.1444],\n",
       "        [ 56.8258,  70.5243],\n",
       "        [ 80.3470,  98.0918],\n",
       "        [119.3828, 135.4291],\n",
       "        [ 20.9150,  38.4267],\n",
       "        [101.5624, 116.3035]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predicts = model(inputs)\n",
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 57.,  69.],\n",
       "        [ 80., 102.],\n",
       "        [118., 132.],\n",
       "        [ 21.,  38.],\n",
       "        [104., 118.],\n",
       "        [ 57.,  69.],\n",
       "        [ 82., 100.],\n",
       "        [118., 134.],\n",
       "        [ 20.,  38.],\n",
       "        [102., 120.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing with targets\n",
    "target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the predictions are quite close to our targets. We jave a trained a reasonably good model to predict crop yeilds for apples and oranges by looking at the average temparature, rainfall, and humidity in a region. We can use it to make predictions of crop yeilds for new regions by passing a batch containing a single row of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([53.4929, 67.5010], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([75,63,44.]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts apples are in ~53 tons per hectare, and oranges are ~67 tons per hectare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
